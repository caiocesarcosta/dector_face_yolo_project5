{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzQfFhZibBXIQj9pgiQ4zr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiocesarcosta/dector_face_yolo_project5/blob/main/dector_face_yolo_project5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gxc6c1CvsNFf",
        "outputId": "d57ab943-d4d6-42e3-91ed-343534eb5c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Clonando yolo do github\n",
            "Instalando dependÃªncias\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Requirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (8.3.77)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (75.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.14)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (3.0.2)\n",
            "Classe 1 iniciada.\n",
            "Classe 2 iniciada.\n",
            "Detectando face da imagem\n",
            "Bounding boxes: [(0.0, 0.495098, 0.517157, 0.980392, 0.955882)]\n",
            "Apresentando Bounding Boxes\n",
            "Formato da imagem JPEG e tamanho (204, 204)\n",
            "Found 56 images belonging to 1 classes.\n",
            "Found 56 images belonging to 1 classes.\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Modelo treinado\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'FaceClassifier' object has no attribute 'validation_generator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-37ba4edcc235>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m \u001b[0mreconhecerImagem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-37ba4edcc235>\u001b[0m in \u001b[0;36mreconhecerImagem\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   \u001b[0;31m#Apresenta o resultado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m   \u001b[0mclassificacao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassificacao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Apresentado Resultado do modelo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FaceClassifier' object has no attribute 'validation_generator'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import shutil\n",
        "import yaml\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ConfiguraÃ§Ãµes\n",
        "img_width, img_height = 224, 224 # Redimensiona as imagens para este tamanho\n",
        "face_detection_threshold = 0.5 # Limiar de confianÃ§a para detecÃ§Ã£o facial\n",
        "drive.mount('/content/drive', force_remount=True) # Conecta ao google drive para acessar os arquivos\n",
        "\n",
        "class FaceDetector:\n",
        "    def __init__(self, img_width):\n",
        "        self.repoYoloGitHub = 'https://github.com/ultralytics/yolov5'\n",
        "        self.pretrained_weights = 'yolov5s.pt'  # Usa o modelo pequeno (mais rÃ¡pido)\n",
        "        self.img_width = img_width\n",
        "        self.cloneRepoistoryYoloGitHub()\n",
        "        self.installDependency()\n",
        "\n",
        "    def cloneRepoistoryYoloGitHub(self):\n",
        "        \"\"\"Clone the yolo repository, from gitHub.\"\"\"\n",
        "        print (\"Clonando yolo do github\")\n",
        "        if not os.path.exists('/content/yolov5'):\n",
        "           clone_command = f\"git clone {self.repoYoloGitHub}\"\n",
        "           os.system(clone_command)  # Use os.system para executar o comando\n",
        "\n",
        "    def installDependency(self):\n",
        "      \"\"\"Install the dependecies.\"\"\"\n",
        "      print (\"Instalando dependÃªncias\")\n",
        "      if os.path.exists('/content/yolov5'):\n",
        "        os.chdir('/content/yolov5')\n",
        "        !pip install -r requirements.txt\n",
        "\n",
        "    def detectFaces(self, source, conf_thres=0.5):\n",
        "        \"\"\"Detecta faces em uma imagem e retorna os bounding boxes.\"\"\"\n",
        "        print (\"Detectando face da imagem\")\n",
        "        detect_command = f\"python detect.py --weights {self.pretrained_weights} --img {self.img_width} --conf {conf_thres} --source \\\"{source}\\\" --save-txt\"\n",
        "        os.system(detect_command)  # Use os.system\n",
        "\n",
        "        return self.parseYOLOResults(source) # carrega para a funÃ§Ã£o as informaÃ§Ãµes da imagem\n",
        "\n",
        "    def parseYOLOResults(self, source):\n",
        "        \"\"\"Analisa os resultados do YOLO e retorna os bounding boxes.\"\"\"\n",
        "        results_dir = '/content/yolov5/runs/detect/exp/labels' # Garante que estamos procurando no local correto\n",
        "        base_name = os.path.basename(source).split('.')[0]\n",
        "        results_file = os.path.join(results_dir, base_name + '.txt')\n",
        "        if not os.path.exists(results_file):\n",
        "            print (f\"Aviso: Nenhum objeto detectado {results_dir}!\")\n",
        "            return [] # Retorna uma lista vazia se nenhum arquivo for encontrado\n",
        "\n",
        "        bounding_boxes = []\n",
        "        try:\n",
        "            with open(results_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    data = line.strip().split()\n",
        "                    if len(data) == 5:\n",
        "                        class_id, x_center, y_center, width, height = map(float, data)\n",
        "                        bounding_boxes.append((class_id, x_center, y_center, width, height))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Arquivo de resultados nÃ£o encontrado: {results_file}\")\n",
        "        return bounding_boxes\n",
        "\n",
        "class FaceClassifier:\n",
        "    def __init__(self, train_dir, validation_dir, img_width, img_height, num_classes, epochs=10, batch_size=32):\n",
        "      self.train_dir = train_dir\n",
        "      self.validation_dir = validation_dir\n",
        "      self.img_width = img_width\n",
        "      self.img_height = img_height\n",
        "      self.num_classes = num_classes\n",
        "      self.epochs = epochs\n",
        "      self.batch_size = batch_size\n",
        "      self.model = self.createModel()\n",
        "      self.compileModel()\n",
        "\n",
        "    def createModel(self):\n",
        "        \"\"\"Cria o modelo de classificaÃ§Ã£o (CNN).\"\"\"\n",
        "        input_shape = (self.img_width, self.img_height, 3)\n",
        "        model = Sequential([\n",
        "            Conv2D(32, (3, 3), input_shape=input_shape, activation='relu'),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            Conv2D(64, (3, 3), activation='relu'),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            Flatten(),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(self.num_classes, activation='softmax') # Camada de saÃ­da\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def compileModel(self):\n",
        "        \"\"\"Compila o modelo para treinamento.\"\"\"\n",
        "        self.model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    def trainModel(self):\n",
        "        \"\"\"Treina o modelo usando os dados do Google Drive.\"\"\"\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1. / 255,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            rotation_range=20)  # Adiciona rotaÃ§Ã£o para aumentar a variabilidade\n",
        "\n",
        "        test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            self.train_dir,\n",
        "            target_size=(self.img_width, self.img_height),\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode='categorical')  # Usar categorical para mÃºltiplas classes\n",
        "\n",
        "        validation_generator = test_datagen.flow_from_directory(\n",
        "            self.validation_dir,\n",
        "            target_size=(self.img_width, self.img_height),\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode='categorical')  # Usar categorical para mÃºltiplas classes\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            train_generator,\n",
        "            steps_per_epoch=train_generator.samples // self.batch_size,\n",
        "            epochs=self.epochs,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=validation_generator.samples // self.batch_size)\n",
        "\n",
        "        return train_generator.class_indices # retorne o resultado das classes para auxÃ­lio\n",
        "    def evaluateModel(self, validation_generator):\n",
        "      \"\"\"FunÃ§Ã£o para avaliar o modelo treinado, e gerar um report.\"\"\"\n",
        "      # Depois do treinamento, avaliar o modelo:\n",
        "      loss, accuracy = self.model.evaluate(validation_generator)\n",
        "      print(f\"Perda (Loss): {loss:.4f}\")\n",
        "      print(f\"PrecisÃ£o (Accuracy): {accuracy:.4f}\")\n",
        "\n",
        "      # Fazer previsÃµes para construir a matriz de confusÃ£o e exibir as metricas para a anÃ¡lise do resultado\n",
        "      from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "      val_steps_per_epoch = np.math.ceil(float(validation_generator.samples) / batch_size)\n",
        "      Y_pred = self.model.predict(validation_generator, steps=val_steps_per_epoch)\n",
        "      y_pred = np.argmax(Y_pred, axis=1)\n",
        "      print('RelatÃ³rio de ClassificaÃ§Ã£o')\n",
        "      print(classification_report(validation_generator.classes, y_pred, target_names=validation_generator.class_indices.keys()))\n",
        "\n",
        "def crop_face(image, bbox):\n",
        "    \"\"\"Corta uma face da imagem usando as coordenadas do bounding box.\"\"\"\n",
        "    class_id, x_center, y_center, width_norm, height_norm = bbox\n",
        "    left = int((x_center - width_norm / 2) * 150)\n",
        "    top = int((y_center - height_norm / 2) * 150)\n",
        "    right = int((x_center + width_norm / 2) * 150)\n",
        "    bottom = int((y_center + height_norm / 2) * 150)\n",
        "    return image.crop((left, top, right, bottom))\n",
        "\n",
        "def reconhecerImagem(): # juntando as funÃ§Ãµes para gerar o treinamento\n",
        "  \"\"\"Implementa o reconhecimento facil \"\"\"\n",
        "  # ConfiguraÃ§Ãµes\n",
        "  train_dir = '/content/drive/MyDrive/the_big_bang_theory/data/train/' # Pasta com as fotos para o treino\n",
        "  validation_dir = '/content/drive/MyDrive/the_big_bang_theory/data/train/'  # Pasta com as fotos de validaÃ§Ã£o\n",
        "  img_width, img_height = 224, 224 # Redimensiona as imagens para este tamanho\n",
        "  num_classes = len(os.listdir(train_dir)) # Ajuste conforme o nÃºmero de pessoas\n",
        "  batch_size = 32\n",
        "  epochs = 5\n",
        "\n",
        "  # Treinamento do modelo\n",
        "  # Verifique se hÃ¡ imagens no diretÃ³rio de treinamento\n",
        "  if len(os.listdir(train_dir)) == 0:\n",
        "    print(f\"Erro: O diretÃ³rio de treinamento '{train_dir}' estÃ¡ vazio. Verifique o caminho.\")\n",
        "    return  # Sai da funÃ§Ã£o se o diretÃ³rio estiver vazio\n",
        "\n",
        "\n",
        "\n",
        "  # Inicializa o Detector de Faces\n",
        "  detector = FaceDetector(img_width)\n",
        "  print(\"Classe 1 iniciada.\")\n",
        "\n",
        "  # Cria as pastas para o modelo para auxiliar no treinamento\n",
        "  faces_dir = '/content/coco_yolo/faces'\n",
        "\n",
        "  #Verifica se a pasta existe, e cria as pastas\n",
        "  if not os.path.exists(faces_dir):\n",
        "      os.makedirs(faces_dir, exist_ok = True)\n",
        "\n",
        "  # Inicializa o Classificador de Faces\n",
        "  classificacao = FaceClassifier(train_dir, validation_dir, img_width, img_height, num_classes, epochs, batch_size)\n",
        "  print(\"Classe 2 iniciada.\")\n",
        "\n",
        "  # Carrega a imagem de exemplo\n",
        "  #test_image = '/content/yolov5/data/images/bus.jpg'  # imagem de exemplo do yolo\n",
        "  test_image = '/content/drive/MyDrive/the_big_bang_theory/data/train/penny/penny1.jpg' # imagem que estÃ¡ no google drive\n",
        "\n",
        "  # Detecta faces na imagem de teste\n",
        "  bounding_boxes = detector.detectFaces(test_image)\n",
        "\n",
        "  # Imprimi Bounding boxes encontrados\n",
        "  print (f\"Bounding boxes: {bounding_boxes}\")\n",
        "  # Imagens que as funÃ§Ãµes nÃ£o encontrassem o que precisavam, nÃ£o seriam mais exibidas, jÃ¡ que a detecÃ§Ã£o seria feita por um novo teste\n",
        "  if bounding_boxes:\n",
        "      print(\"Apresentando Bounding Boxes\")\n",
        "       # Apresenta os dados, caso positivo\n",
        "      # Carrega a imagem novamente\n",
        "      img_open = Image.open(test_image)\n",
        "      print (f\"Formato da imagem {img_open.format} e tamanho {img_open.size}\")\n",
        "  else:\n",
        "      print(\"Nenhuma anotaÃ§Ã£o foi encontrada nessa imagem, tente outras imagens. O programa se encerrarÃ¡.\")\n",
        "      return\n",
        "\n",
        "  # Treinamento do modelo\n",
        "  class_name = classificacao.trainModel()\n",
        "  print(\"Modelo treinado\")\n",
        "\n",
        "  #Apresenta o resultado\n",
        "  classificacao.evaluateModel(classificacao.validation_generator)\n",
        "  print(\"Apresentado Resultado do modelo\")\n",
        "\n",
        "  # Corta as faces e as classifica\n",
        "  for i, bbox in enumerate(bounding_boxes):\n",
        "      #Corta a imagem com as dimensÃµes\n",
        "      face_crop = crop_face(img_open, bbox)\n",
        "       # Converte para RGB, para evitar erros\n",
        "      if face_crop.mode != \"RGB\":\n",
        "        face_crop = face_crop.convert(\"RGB\")\n",
        "      #Salva as imagens na pasta\n",
        "      face_crop.save(os.path.join(faces_dir, f\"face_{i}.jpg\"))\n",
        "      print (f\"Imagem {i} salva\")\n",
        "\n",
        "#FunÃ§Ã£o para cortar as imagens.\n",
        "def crop_face(image, bbox):\n",
        "    \"\"\"Corta uma face da imagem usando as coordenadas do bounding box.\"\"\"\n",
        "    class_id, x_center, y_center, width_norm, height_norm = bbox\n",
        "    left = int((x_center - width_norm / 2) * 150)\n",
        "    top = int((y_center - height_norm / 2) * 150)\n",
        "    right = int((x_center + width_norm / 2) * 150)\n",
        "    bottom = int((y_center + height_norm / 2) * 150)\n",
        "    return image.crop((left, top, right, bottom))\n",
        "\n",
        "reconhecerImagem()\n"
      ]
    }
  ]
}